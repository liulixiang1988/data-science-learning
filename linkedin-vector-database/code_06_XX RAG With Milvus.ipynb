{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b94809",
   "metadata": {},
   "source": [
    "### 06.01 Setting up Milvus for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07a8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases:  ['default', 'cache_db']\n",
      "Creating database : rag_db\n"
     ]
    }
   ],
   "source": [
    "#Create the Connection and database for RAG\n",
    "from pymilvus import connections\n",
    "from pymilvus import db,Collection\n",
    "\n",
    "from pymilvus import utility\n",
    "\n",
    "connections.add_connection(\n",
    "    rag_conn={\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": \"19530\",\n",
    "        \"username\" : \"username\",\n",
    "        \"password\" : \"password\"\n",
    "    })\n",
    "\n",
    "conn_name=\"rag_conn\"\n",
    "db_name=\"rag_db\"\n",
    "\n",
    "connections.connect(conn_name)\n",
    "connections.list_connections()\n",
    "\n",
    "current_dbs=db.list_database(using=conn_name)\n",
    "print(\"Databases: \", current_dbs)\n",
    "\n",
    "if ( db_name not in current_dbs):\n",
    "    print(\"Creating database :\", db_name)\n",
    "    resume_db = db.create_database(db_name, using=conn_name) \n",
    "\n",
    "#Switch to the new database\n",
    "db.using_database(db_name, using=conn_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee43eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections:  ['rag_collection']\n",
      "\n",
      " Schema : {'auto_id': False, 'description': 'RAG Schema', 'fields': [{'name': 'chunk_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'rag_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 2048}}, {'name': 'rag_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1536}}], 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "#Create a new collection for RAG\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, Collection\n",
    "import json\n",
    "\n",
    "chunk_id_field = FieldSchema(\n",
    "    name=\"chunk_id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    max_length=32)\n",
    "\n",
    "rag_text_field= FieldSchema(\n",
    "    name=\"rag_text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=2048)\n",
    "\n",
    "rag_embedding_field = FieldSchema(\n",
    "    name=\"rag_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1536 #Define based on embedding used\n",
    ")\n",
    "\n",
    "rag_schema=CollectionSchema(\n",
    "    fields=[chunk_id_field, rag_text_field, rag_embedding_field],\n",
    "    description=\"RAG Schema\",\n",
    "    enable_dynamic_field=True\n",
    ")\n",
    "\n",
    "collection_name=\"rag_collection\"\n",
    "\n",
    "rag_collection=Collection(\n",
    "    name=collection_name,\n",
    "    schema=rag_schema,\n",
    "    using=conn_name,\n",
    "    shard_num=2\n",
    ")\n",
    "\n",
    "from pymilvus import utility\n",
    "print(\"Collections: \", utility.list_collections(using=conn_name))\n",
    "\n",
    "r_collection=Collection(collection_name, using=conn_name)\n",
    "print(\"\\n Schema :\", r_collection.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb7109",
   "metadata": {},
   "source": [
    "### 06.02. Preparing data for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9348040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up the PDF document\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "\n",
    "loader = PDFMinerLoader(\"Large Language Models.pdf\")\n",
    "pdf_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dad2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks : 23\n",
      "Sample chunk text:  As autoregressive language models, they work by taking an input text and repeatedly predicting the \n",
      "next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to \n",
      "accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to \n",
      "achieve similar results. They are thought to acquire knowledge about syntax, semantics and \n",
      "\"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the\n"
     ]
    }
   ],
   "source": [
    "#Split document into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter   =   RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, # Specify the character chunk size\n",
    "    chunk_overlap=32, # \"Allowed\" Overlap across chunks\n",
    "    length_function=len # Function used to evaluate the chunk size (here in terms of characters)\n",
    ")\n",
    "\n",
    "pdf_docs    =   text_splitter.split_documents(pdf_docs)\n",
    "\n",
    "#Create a list of chunks\n",
    "rag_text =[]\n",
    "for i in pdf_docs:\n",
    "    rag_text.append(i.page_content)\n",
    "    \n",
    "print(\"Total chunks :\", len(rag_text))\n",
    "print(\"Sample chunk text: \", rag_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84373100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "openai_api_key=\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "rag_embedding=[embeddings_model.embed_query(i) \n",
    "                  for i in rag_text]\n",
    "\n",
    "#Create chunk IDs \n",
    "record_ids=[i for i in range(len(rag_text))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f0bf8",
   "metadata": {},
   "source": [
    "### 06.03. Populating the Milvus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15e054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_rows': 23, 'indexed_rows': 23, 'pending_index_rows': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_data=[record_ids, rag_text, rag_embedding]\n",
    "\n",
    "i_collection = Collection(collection_name, using=conn_name)\n",
    "\n",
    "#Insert the records\n",
    "mr=i_collection.insert(insert_data)\n",
    "#Flush the inserted records\n",
    "i_collection.flush()\n",
    "\n",
    "#Build an index on the embedding field\n",
    "index_params = {\n",
    "    \"metric_type\":\"L2\",\n",
    "    \"index_type\":\"IVF_FLAT\",\n",
    "    \"params\" :{\"nlist\":1024}\n",
    "}\n",
    "\n",
    "i_collection.create_index(\n",
    "    field_name=\"rag_embedding\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "utility.index_building_progress(collection_name, using=conn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620eb45a",
   "metadata": {},
   "source": [
    "### 06.04 Answering questions with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268f107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result : id: 8, distance: 0.20583292841911316, entity: {'rag_text': 'Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced \\ntowards one gender over another. This bias typically arises from the data on which these models are \\ntrained. For example, large language models often assign roles and characteristics based on \\ntraditional gender norms; it might associate nurses or secretaries predominantly with women and \\nengineers or CEOs with men.'}\n"
     ]
    }
   ],
   "source": [
    "#The retrieval process\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 20, \"radius\":0.5}\n",
    "}\n",
    "\n",
    "query = \"What is gender bias?\"\n",
    "search_embed=embeddings_model.embed_query(query)\n",
    "#print(search_embed)\n",
    "\n",
    "q_collection = Collection(collection_name, using=conn_name)\n",
    "q_collection.load()\n",
    "\n",
    "results=q_collection.search(\n",
    "    data=[search_embed],\n",
    "    anns_field=\"rag_embedding\",\n",
    "    param=search_params,\n",
    "    limit=3, #Get top 3 results only\n",
    "    expr=None,\n",
    "    output_fields=[\"rag_text\"],\n",
    "    consistency_level=\"Strong\"\n",
    ")\n",
    "\n",
    "print(\"Top result :\", results[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece4840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on only the context provided, answer the query below:  Context: ['Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced \\ntowards one gender over another. This bias typically arises from the data on which these models are \\ntrained. For example, large language models often assign roles and characteristics based on \\ntraditional gender norms; it might associate nurses or secretaries predominantly with women and \\nengineers or CEOs with men.', 'Language bias refers a type of statistical sampling bias tied to the language of a query that leads to \\n\"a systematic deviation in sampling information that prevents it from accurately representing the \\ntrue coverage of topics and views available in their repository.\" Luo et al. show that current large \\nlanguage models, as they are predominately trained on English-language data, often present the \\nAnglo-American views as truth, while systematically downplaying non-English perspectives as', 'Beyond gender and race, these models can reinforce a wide range of stereotypes, including those \\nbased on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or \\ncaricature groups of people, sometimes in harmful or derogatory ways.']\n",
      "\n",
      " Query: What is gender bias?\n"
     ]
    }
   ],
   "source": [
    "#Prepare prompt for LLM\n",
    "\n",
    "context=[]\n",
    "\n",
    "#Append all returned chunks\n",
    "for i in range(len(results[0])):\n",
    "    context.append(results[0][i].entity.get(\"rag_text\"))\n",
    "\n",
    "#Create a prompt\n",
    "prompt= (\"Based on only the context provided, answer the query below: \"\n",
    "        + \" Context: \" + str(context)\n",
    "        + \"\\n\\n Query: \" + query)\n",
    "        \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc5f933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gender bias is the tendency of models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained, and can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\n"
     ]
    }
   ],
   "source": [
    "#Generate with LLM\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm= OpenAI(temperature=0., model=\"text-davinci-003\")\n",
    "\n",
    "completion=llm(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4632ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
